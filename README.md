# Welcome to the Video Captioning 

This project was a research project, where our goal was to design a model to create captions from the given Video file. We specifically took the videos which had no audio in it. We used the distance-based approach to predict the action between the objects. 


# Tools
  1. Python
  2. OpenCV
  3. Yolo
  4. Chatgpt

# How to run: 
 1. Locate the main.py file and edit the line 34 to feed the input video. 
 2. It has finished running the main.py, copy the result from the terminal.
 3. Now, run the project.py file on Google Collab and use the input as the result acquired from the main.py



# overview details and presentation Link : [Link](https://www.canva.com/design/DAFkj081RDk/O6-dPVDsAlz4MzcnVGcXkg/edit?utm_content=DAFkj081RDk&utm_campaign=designshare&utm_medium=link2&utm_source=sharebutton)
  
# Result: 
<img width="863" alt="Screen Shot 2024-07-11 at 5 36 36 pm" src="https://github.com/a1819644/Video_labelling/assets/88136301/777d7735-a9f5-4aa3-bc67-6da25a14b294">

# Limits
<img width="766" alt="Screen Shot 2024-07-11 at 5 37 49 pm" src="https://github.com/a1819644/Video_labelling/assets/88136301/a0b1bfc1-de8a-4a96-b506-412e87772b72">
:
